// SPDX-FileCopyrightText: Â© 2025 DSLab - Fondazione Bruno Kessler
// SPDX-FileCopyrightText: Copyright (c) 2020-2024, NVIDIA CORPORATION & AFFILIATES
//
// SPDX-License-Identifier: Apache-2.0

// Open Inference Protocol v2 - gRPC Service Definition
// Based on: https://github.com/kserve/open-inference-protocol

// Code generated by protoc-gen-go. DO NOT EDIT.
// versions:
// 	protoc-gen-go v1.36.11
// 	protoc        v6.33.4
// source: pkg/proto/inference/v2/grpc_service.proto

package inference

import (
	protoreflect "google.golang.org/protobuf/reflect/protoreflect"
	protoimpl "google.golang.org/protobuf/runtime/protoimpl"
	reflect "reflect"
	sync "sync"
	unsafe "unsafe"
)

const (
	// Verify that this generated code is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(20 - protoimpl.MinVersion)
	// Verify that runtime/protoimpl is sufficiently up-to-date.
	_ = protoimpl.EnforceVersion(protoimpl.MaxVersion - 20)
)

// @@
// @@.. cpp:var:: message ServerLiveRequest
// @@
// @@   Request message for ServerLive.
// @@
type ServerLiveRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerLiveRequest) Reset() {
	*x = ServerLiveRequest{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[0]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerLiveRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerLiveRequest) ProtoMessage() {}

func (x *ServerLiveRequest) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[0]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerLiveRequest.ProtoReflect.Descriptor instead.
func (*ServerLiveRequest) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{0}
}

// @@
// @@.. cpp:var:: message ServerLiveResponse
// @@
// @@   Response message for ServerLive.
// @@
type ServerLiveResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: bool live
	// @@
	// @@     True if the inference server is live, false otherwise.
	// @@
	Live          bool `protobuf:"varint,1,opt,name=live,proto3" json:"live,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerLiveResponse) Reset() {
	*x = ServerLiveResponse{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[1]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerLiveResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerLiveResponse) ProtoMessage() {}

func (x *ServerLiveResponse) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[1]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerLiveResponse.ProtoReflect.Descriptor instead.
func (*ServerLiveResponse) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{1}
}

func (x *ServerLiveResponse) GetLive() bool {
	if x != nil {
		return x.Live
	}
	return false
}

// @@
// @@.. cpp:var:: message ServerReadyRequest
// @@
// @@   Request message for ServerReady.
// @@
type ServerReadyRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerReadyRequest) Reset() {
	*x = ServerReadyRequest{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[2]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerReadyRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerReadyRequest) ProtoMessage() {}

func (x *ServerReadyRequest) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[2]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerReadyRequest.ProtoReflect.Descriptor instead.
func (*ServerReadyRequest) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{2}
}

// @@
// @@.. cpp:var:: message ServerReadyResponse
// @@
// @@   Response message for ServerReady.
// @@
type ServerReadyResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: bool ready
	// @@
	// @@     True if the inference server is ready, false otherwise.
	// @@
	Ready         bool `protobuf:"varint,1,opt,name=ready,proto3" json:"ready,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerReadyResponse) Reset() {
	*x = ServerReadyResponse{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[3]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerReadyResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerReadyResponse) ProtoMessage() {}

func (x *ServerReadyResponse) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[3]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerReadyResponse.ProtoReflect.Descriptor instead.
func (*ServerReadyResponse) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{3}
}

func (x *ServerReadyResponse) GetReady() bool {
	if x != nil {
		return x.Ready
	}
	return false
}

// @@
// @@.. cpp:var:: message ModelReadyRequest
// @@
// @@   Request message for ModelReady.
// @@
type ModelReadyRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: string name
	// @@
	// @@     The name of the model to check for readiness.
	// @@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// @@  .. cpp:var:: string version
	// @@
	// @@     The version of the model to check for readiness. If not given the
	// @@     server will choose a version based on the model and internal policy.
	// @@
	Version       string `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelReadyRequest) Reset() {
	*x = ModelReadyRequest{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[4]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelReadyRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelReadyRequest) ProtoMessage() {}

func (x *ModelReadyRequest) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[4]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelReadyRequest.ProtoReflect.Descriptor instead.
func (*ModelReadyRequest) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{4}
}

func (x *ModelReadyRequest) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelReadyRequest) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

// @@
// @@.. cpp:var:: message ModelReadyResponse
// @@
// @@   Response message for ModelReady.
// @@
type ModelReadyResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: bool ready
	// @@
	// @@     True if the model is ready, false otherwise.
	// @@
	Ready         bool `protobuf:"varint,1,opt,name=ready,proto3" json:"ready,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelReadyResponse) Reset() {
	*x = ModelReadyResponse{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[5]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelReadyResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelReadyResponse) ProtoMessage() {}

func (x *ModelReadyResponse) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[5]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelReadyResponse.ProtoReflect.Descriptor instead.
func (*ModelReadyResponse) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{5}
}

func (x *ModelReadyResponse) GetReady() bool {
	if x != nil {
		return x.Ready
	}
	return false
}

// @@
// @@.. cpp:var:: message ServerMetadataRequest
// @@
// @@   Request message for ServerMetadata.
// @@
type ServerMetadataRequest struct {
	state         protoimpl.MessageState `protogen:"open.v1"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerMetadataRequest) Reset() {
	*x = ServerMetadataRequest{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[6]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerMetadataRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerMetadataRequest) ProtoMessage() {}

func (x *ServerMetadataRequest) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[6]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerMetadataRequest.ProtoReflect.Descriptor instead.
func (*ServerMetadataRequest) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{6}
}

// @@
// @@.. cpp:var:: message ServerMetadataResponse
// @@
// @@   Response message for ServerMetadata.
// @@
type ServerMetadataResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: string name
	// @@
	// @@     The server name.
	// @@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// @@  .. cpp:var:: string version
	// @@
	// @@     The server version.
	// @@
	Version string `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	// @@  .. cpp:var:: repeated string extensions
	// @@
	// @@     The extensions supported by the server.
	// @@
	Extensions    []string `protobuf:"bytes,3,rep,name=extensions,proto3" json:"extensions,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ServerMetadataResponse) Reset() {
	*x = ServerMetadataResponse{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[7]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ServerMetadataResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ServerMetadataResponse) ProtoMessage() {}

func (x *ServerMetadataResponse) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[7]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ServerMetadataResponse.ProtoReflect.Descriptor instead.
func (*ServerMetadataResponse) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{7}
}

func (x *ServerMetadataResponse) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ServerMetadataResponse) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

func (x *ServerMetadataResponse) GetExtensions() []string {
	if x != nil {
		return x.Extensions
	}
	return nil
}

// @@
// @@.. cpp:var:: message ModelMetadataRequest
// @@
// @@   Request message for ModelMetadata.
// @@
type ModelMetadataRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: string name
	// @@
	// @@     The name of the model.
	// @@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// @@  .. cpp:var:: string version
	// @@
	// @@     The version of the model to check for readiness. If not given the
	// @@     server will choose a version based on the model and internal policy.
	// @@
	Version       string `protobuf:"bytes,2,opt,name=version,proto3" json:"version,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelMetadataRequest) Reset() {
	*x = ModelMetadataRequest{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[8]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelMetadataRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelMetadataRequest) ProtoMessage() {}

func (x *ModelMetadataRequest) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[8]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelMetadataRequest.ProtoReflect.Descriptor instead.
func (*ModelMetadataRequest) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{8}
}

func (x *ModelMetadataRequest) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelMetadataRequest) GetVersion() string {
	if x != nil {
		return x.Version
	}
	return ""
}

// @@
// @@.. cpp:var:: message ModelMetadataResponse
// @@
// @@   Response message for ModelMetadata.
// @@
type ModelMetadataResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: string name
	// @@
	// @@     The model name.
	// @@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// @@  .. cpp:var:: repeated string versions
	// @@
	// @@     The versions of the model available on the server.
	// @@
	Versions []string `protobuf:"bytes,2,rep,name=versions,proto3" json:"versions,omitempty"`
	// @@  .. cpp:var:: string platform
	// @@
	// @@     The model's platform.
	// @@
	Platform string `protobuf:"bytes,3,opt,name=platform,proto3" json:"platform,omitempty"`
	// @@  .. cpp:var:: repeated TensorMetadata inputs
	// @@
	// @@     The model's inputs.
	// @@
	Inputs []*TensorMetadata `protobuf:"bytes,4,rep,name=inputs,proto3" json:"inputs,omitempty"`
	// @@  .. cpp:var:: repeated TensorMetadata outputs
	// @@
	// @@     The model's outputs.
	// @@
	Outputs       []*TensorMetadata `protobuf:"bytes,5,rep,name=outputs,proto3" json:"outputs,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelMetadataResponse) Reset() {
	*x = ModelMetadataResponse{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[9]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelMetadataResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelMetadataResponse) ProtoMessage() {}

func (x *ModelMetadataResponse) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[9]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelMetadataResponse.ProtoReflect.Descriptor instead.
func (*ModelMetadataResponse) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{9}
}

func (x *ModelMetadataResponse) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelMetadataResponse) GetVersions() []string {
	if x != nil {
		return x.Versions
	}
	return nil
}

func (x *ModelMetadataResponse) GetPlatform() string {
	if x != nil {
		return x.Platform
	}
	return ""
}

func (x *ModelMetadataResponse) GetInputs() []*TensorMetadata {
	if x != nil {
		return x.Inputs
	}
	return nil
}

func (x *ModelMetadataResponse) GetOutputs() []*TensorMetadata {
	if x != nil {
		return x.Outputs
	}
	return nil
}

// @@
// @@.. cpp:var:: message ModelInferRequest
// @@
// @@   Request message for ModelInfer.
// @@
type ModelInferRequest struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: string model_name
	// @@
	// @@     The name of the model to use for inferencing.
	// @@
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// @@  .. cpp:var:: string model_version
	// @@
	// @@     The version of the model to use for inferencing. If not given the
	// @@     server will choose a version based on the model and internal policy.
	// @@
	ModelVersion string `protobuf:"bytes,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	// @@  .. cpp:var:: string id
	// @@
	// @@     Optional identifier for the request. If specified will be
	// @@     returned in the response.
	// @@
	Id string `protobuf:"bytes,3,opt,name=id,proto3" json:"id,omitempty"`
	// @@  .. cpp:var:: map<string,InferParameter> parameters
	// @@
	// @@     Optional inference parameters.
	// @@
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// @@  .. cpp:var:: repeated InferInputTensor inputs
	// @@
	// @@     The input tensors for the inference.
	// @@
	Inputs []*ModelInferRequest_InferInputTensor `protobuf:"bytes,5,rep,name=inputs,proto3" json:"inputs,omitempty"`
	// @@  .. cpp:var:: repeated InferRequestedOutputTensor outputs
	// @@
	// @@     The requested output tensors for the inference. Optional, if not
	// @@     specified all outputs specified in the model config will be returned.
	// @@
	Outputs []*ModelInferRequest_InferRequestedOutputTensor `protobuf:"bytes,6,rep,name=outputs,proto3" json:"outputs,omitempty"`
	// @@  .. cpp:var:: repeated bytes raw_input_contents
	// @@
	// @@     The data contained in an input tensor can be represented in "raw"
	// @@     bytes form or in the repeated type that matches the tensor's data
	// @@     type. Using the "raw" bytes form will typically allow higher
	// @@     performance due to the way protobuf allocation and reuse interacts
	// @@     with GRPC. For example, see
	// @@     https://github.com/grpc/grpc/issues/23231.
	// @@
	// @@     To use the raw representation 'raw_input_contents' must be
	// @@     initialized with data for each tensor in the same order as
	// @@     'inputs'. For each tensor, the size of this content must match
	// @@     what is expected by the tensor's shape and data type. The raw
	// @@     data must be the flattened, one-dimensional, row-major order of
	// @@     the tensor elements without any stride or padding between the
	// @@     elements. Note that the FP16 data type must be represented as raw
	// @@     content as there is no specific data type for a 16-bit float
	// @@     type.
	// @@
	// @@     If this field is specified then InferInputTensor::contents must
	// @@     not be specified for any input tensor.
	// @@
	RawInputContents [][]byte `protobuf:"bytes,7,rep,name=raw_input_contents,json=rawInputContents,proto3" json:"raw_input_contents,omitempty"`
	unknownFields    protoimpl.UnknownFields
	sizeCache        protoimpl.SizeCache
}

func (x *ModelInferRequest) Reset() {
	*x = ModelInferRequest{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[10]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInferRequest) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInferRequest) ProtoMessage() {}

func (x *ModelInferRequest) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[10]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInferRequest.ProtoReflect.Descriptor instead.
func (*ModelInferRequest) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{10}
}

func (x *ModelInferRequest) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *ModelInferRequest) GetModelVersion() string {
	if x != nil {
		return x.ModelVersion
	}
	return ""
}

func (x *ModelInferRequest) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *ModelInferRequest) GetParameters() map[string]*InferParameter {
	if x != nil {
		return x.Parameters
	}
	return nil
}

func (x *ModelInferRequest) GetInputs() []*ModelInferRequest_InferInputTensor {
	if x != nil {
		return x.Inputs
	}
	return nil
}

func (x *ModelInferRequest) GetOutputs() []*ModelInferRequest_InferRequestedOutputTensor {
	if x != nil {
		return x.Outputs
	}
	return nil
}

func (x *ModelInferRequest) GetRawInputContents() [][]byte {
	if x != nil {
		return x.RawInputContents
	}
	return nil
}

// @@
// @@.. cpp:var:: message ModelInferResponse
// @@
// @@   Response message for ModelInfer.
// @@
type ModelInferResponse struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: string model_name
	// @@
	// @@     The name of the model used for inference.
	// @@
	ModelName string `protobuf:"bytes,1,opt,name=model_name,json=modelName,proto3" json:"model_name,omitempty"`
	// @@  .. cpp:var:: string model_version
	// @@
	// @@     The version of the model used for inference.
	// @@
	ModelVersion string `protobuf:"bytes,2,opt,name=model_version,json=modelVersion,proto3" json:"model_version,omitempty"`
	// @@  .. cpp:var:: string id
	// @@
	// @@     The id of the inference request if one was specified.
	// @@
	Id string `protobuf:"bytes,3,opt,name=id,proto3" json:"id,omitempty"`
	// @@  .. cpp:var:: map<string,InferParameter> parameters
	// @@
	// @@     Optional inference response parameters.
	// @@
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// @@  .. cpp:var:: repeated InferOutputTensor outputs
	// @@
	// @@     The output tensors holding inference results.
	// @@
	Outputs []*ModelInferResponse_InferOutputTensor `protobuf:"bytes,5,rep,name=outputs,proto3" json:"outputs,omitempty"`
	// @@  .. cpp:var:: repeated bytes raw_output_contents
	// @@
	// @@     The data contained in an output tensor can be represented in
	// @@     "raw" bytes form or in the repeated type that matches the
	// @@     tensor's data type. Using the "raw" bytes form will typically
	// @@     allow higher performance due to the way protobuf allocation and
	// @@     reuse interacts with GRPC. For example, see
	// @@     https://github.com/grpc/grpc/issues/23231.
	// @@
	// @@     To use the raw representation 'raw_output_contents' must be
	// @@     initialized with data for each tensor in the same order as
	// @@     'outputs'. For each tensor, the size of this content must match
	// @@     what is expected by the tensor's shape and data type. The raw
	// @@     data must be the flattened, one-dimensional, row-major order of
	// @@     the tensor elements without any stride or padding between the
	// @@     elements. Note that the FP16 data type must be represented as raw
	// @@     content as there is no specific data type for a 16-bit float
	// @@     type.
	// @@
	// @@     If this field is specified then InferOutputTensor::contents must
	// @@     not be specified for any output tensor.
	// @@
	RawOutputContents [][]byte `protobuf:"bytes,6,rep,name=raw_output_contents,json=rawOutputContents,proto3" json:"raw_output_contents,omitempty"`
	unknownFields     protoimpl.UnknownFields
	sizeCache         protoimpl.SizeCache
}

func (x *ModelInferResponse) Reset() {
	*x = ModelInferResponse{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[11]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInferResponse) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInferResponse) ProtoMessage() {}

func (x *ModelInferResponse) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[11]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInferResponse.ProtoReflect.Descriptor instead.
func (*ModelInferResponse) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{11}
}

func (x *ModelInferResponse) GetModelName() string {
	if x != nil {
		return x.ModelName
	}
	return ""
}

func (x *ModelInferResponse) GetModelVersion() string {
	if x != nil {
		return x.ModelVersion
	}
	return ""
}

func (x *ModelInferResponse) GetId() string {
	if x != nil {
		return x.Id
	}
	return ""
}

func (x *ModelInferResponse) GetParameters() map[string]*InferParameter {
	if x != nil {
		return x.Parameters
	}
	return nil
}

func (x *ModelInferResponse) GetOutputs() []*ModelInferResponse_InferOutputTensor {
	if x != nil {
		return x.Outputs
	}
	return nil
}

func (x *ModelInferResponse) GetRawOutputContents() [][]byte {
	if x != nil {
		return x.RawOutputContents
	}
	return nil
}

// @@
// @@.. cpp:var:: message TensorMetadata
// @@
// @@   Metadata for a tensor.
// @@
type TensorMetadata struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: string name
	// @@
	// @@     The tensor name.
	// @@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// @@  .. cpp:var:: string datatype
	// @@
	// @@     The tensor data type.
	// @@
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	// @@  .. cpp:var:: repeated int64 shape
	// @@
	// @@     The tensor shape. A variable-size dimension is represented
	// @@     by a -1 value.
	// @@
	Shape         []int64 `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *TensorMetadata) Reset() {
	*x = TensorMetadata{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[12]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *TensorMetadata) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*TensorMetadata) ProtoMessage() {}

func (x *TensorMetadata) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[12]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use TensorMetadata.ProtoReflect.Descriptor instead.
func (*TensorMetadata) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{12}
}

func (x *TensorMetadata) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *TensorMetadata) GetDatatype() string {
	if x != nil {
		return x.Datatype
	}
	return ""
}

func (x *TensorMetadata) GetShape() []int64 {
	if x != nil {
		return x.Shape
	}
	return nil
}

// @@
// @@.. cpp:var:: message InferParameter
// @@
// @@   An inference parameter value.
// @@
type InferParameter struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: oneof parameter_choice
	// @@
	// @@     The parameter value can be a string, an int64, a boolean
	// @@     or a message specific to a predefined parameter.
	// @@
	//
	// Types that are valid to be assigned to ParameterChoice:
	//
	//	*InferParameter_BoolParam
	//	*InferParameter_Int64Param
	//	*InferParameter_StringParam
	ParameterChoice isInferParameter_ParameterChoice `protobuf_oneof:"parameter_choice"`
	unknownFields   protoimpl.UnknownFields
	sizeCache       protoimpl.SizeCache
}

func (x *InferParameter) Reset() {
	*x = InferParameter{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[13]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferParameter) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferParameter) ProtoMessage() {}

func (x *InferParameter) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[13]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferParameter.ProtoReflect.Descriptor instead.
func (*InferParameter) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{13}
}

func (x *InferParameter) GetParameterChoice() isInferParameter_ParameterChoice {
	if x != nil {
		return x.ParameterChoice
	}
	return nil
}

func (x *InferParameter) GetBoolParam() bool {
	if x != nil {
		if x, ok := x.ParameterChoice.(*InferParameter_BoolParam); ok {
			return x.BoolParam
		}
	}
	return false
}

func (x *InferParameter) GetInt64Param() int64 {
	if x != nil {
		if x, ok := x.ParameterChoice.(*InferParameter_Int64Param); ok {
			return x.Int64Param
		}
	}
	return 0
}

func (x *InferParameter) GetStringParam() string {
	if x != nil {
		if x, ok := x.ParameterChoice.(*InferParameter_StringParam); ok {
			return x.StringParam
		}
	}
	return ""
}

type isInferParameter_ParameterChoice interface {
	isInferParameter_ParameterChoice()
}

type InferParameter_BoolParam struct {
	// @@    .. cpp:var:: bool bool_param
	// @@
	// @@       A boolean parameter value.
	// @@
	BoolParam bool `protobuf:"varint,1,opt,name=bool_param,json=boolParam,proto3,oneof"`
}

type InferParameter_Int64Param struct {
	// @@    .. cpp:var:: int64 int64_param
	// @@
	// @@       An int64 parameter value.
	// @@
	Int64Param int64 `protobuf:"varint,2,opt,name=int64_param,json=int64Param,proto3,oneof"`
}

type InferParameter_StringParam struct {
	// @@    .. cpp:var:: string string_param
	// @@
	// @@       A string parameter value.
	// @@
	StringParam string `protobuf:"bytes,3,opt,name=string_param,json=stringParam,proto3,oneof"`
}

func (*InferParameter_BoolParam) isInferParameter_ParameterChoice() {}

func (*InferParameter_Int64Param) isInferParameter_ParameterChoice() {}

func (*InferParameter_StringParam) isInferParameter_ParameterChoice() {}

// @@
// @@.. cpp:var:: message InferTensorContents
// @@
// @@   The data contained in a tensor represented by the repeated type
// @@   that matches the tensor's data type. Protobuf oneof is not used
// @@   because oneofs cannot contain repeated fields.
// @@
type InferTensorContents struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@  .. cpp:var:: repeated bool bool_contents
	// @@
	// @@     Representation for BOOL data type. The size must match what is
	// @@     expected by the tensor's shape. The contents must be the flattened,
	// @@     one-dimensional, row-major order of the tensor elements.
	// @@
	BoolContents []bool `protobuf:"varint,1,rep,packed,name=bool_contents,json=boolContents,proto3" json:"bool_contents,omitempty"`
	// @@  .. cpp:var:: repeated int32 int_contents
	// @@
	// @@     Representation for INT8, INT16, and INT32 data types. The size
	// @@     must match what is expected by the tensor's shape. The contents
	// @@     must be the flattened, one-dimensional, row-major order of the
	// @@     tensor elements.
	// @@
	IntContents []int32 `protobuf:"varint,2,rep,packed,name=int_contents,json=intContents,proto3" json:"int_contents,omitempty"`
	// @@  .. cpp:var:: repeated int64 int64_contents
	// @@
	// @@     Representation for INT64 data type. The size must match what
	// @@     is expected by the tensor's shape. The contents must be the
	// @@     flattened, one-dimensional, row-major order of the tensor elements.
	// @@
	Int64Contents []int64 `protobuf:"varint,3,rep,packed,name=int64_contents,json=int64Contents,proto3" json:"int64_contents,omitempty"`
	// @@  .. cpp:var:: repeated uint32 uint_contents
	// @@
	// @@     Representation for UINT8, UINT16, and UINT32 data types. The size
	// @@     must match what is expected by the tensor's shape. The contents
	// @@     must be the flattened, one-dimensional, row-major order of the
	// @@     tensor elements.
	// @@
	UintContents []uint32 `protobuf:"varint,4,rep,packed,name=uint_contents,json=uintContents,proto3" json:"uint_contents,omitempty"`
	// @@  .. cpp:var:: repeated uint64 uint64_contents
	// @@
	// @@     Representation for UINT64 data type. The size must match what
	// @@     is expected by the tensor's shape. The contents must be the
	// @@     flattened, one-dimensional, row-major order of the tensor elements.
	// @@
	Uint64Contents []uint64 `protobuf:"varint,5,rep,packed,name=uint64_contents,json=uint64Contents,proto3" json:"uint64_contents,omitempty"`
	// @@  .. cpp:var:: repeated float fp32_contents
	// @@
	// @@     Representation for FP32 data type. The size must match what is
	// @@     expected by the tensor's shape. The contents must be the flattened,
	// @@     one-dimensional, row-major order of the tensor elements.
	// @@
	Fp32Contents []float32 `protobuf:"fixed32,6,rep,packed,name=fp32_contents,json=fp32Contents,proto3" json:"fp32_contents,omitempty"`
	// @@  .. cpp:var:: repeated double fp64_contents
	// @@
	// @@     Representation for FP64 data type. The size must match what is
	// @@     expected by the tensor's shape. The contents must be the flattened,
	// @@     one-dimensional, row-major order of the tensor elements.
	// @@
	Fp64Contents []float64 `protobuf:"fixed64,7,rep,packed,name=fp64_contents,json=fp64Contents,proto3" json:"fp64_contents,omitempty"`
	// @@  .. cpp:var:: repeated bytes bytes_contents
	// @@
	// @@     Representation for BYTES data type. The size must match what is
	// @@     expected by the tensor's shape. The contents must be the flattened,
	// @@     one-dimensional, row-major order of the tensor elements.
	// @@
	BytesContents [][]byte `protobuf:"bytes,8,rep,name=bytes_contents,json=bytesContents,proto3" json:"bytes_contents,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *InferTensorContents) Reset() {
	*x = InferTensorContents{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[14]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *InferTensorContents) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*InferTensorContents) ProtoMessage() {}

func (x *InferTensorContents) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[14]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use InferTensorContents.ProtoReflect.Descriptor instead.
func (*InferTensorContents) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{14}
}

func (x *InferTensorContents) GetBoolContents() []bool {
	if x != nil {
		return x.BoolContents
	}
	return nil
}

func (x *InferTensorContents) GetIntContents() []int32 {
	if x != nil {
		return x.IntContents
	}
	return nil
}

func (x *InferTensorContents) GetInt64Contents() []int64 {
	if x != nil {
		return x.Int64Contents
	}
	return nil
}

func (x *InferTensorContents) GetUintContents() []uint32 {
	if x != nil {
		return x.UintContents
	}
	return nil
}

func (x *InferTensorContents) GetUint64Contents() []uint64 {
	if x != nil {
		return x.Uint64Contents
	}
	return nil
}

func (x *InferTensorContents) GetFp32Contents() []float32 {
	if x != nil {
		return x.Fp32Contents
	}
	return nil
}

func (x *InferTensorContents) GetFp64Contents() []float64 {
	if x != nil {
		return x.Fp64Contents
	}
	return nil
}

func (x *InferTensorContents) GetBytesContents() [][]byte {
	if x != nil {
		return x.BytesContents
	}
	return nil
}

// @@
// @@  .. cpp:var:: message InferInputTensor
// @@
// @@     An input tensor for an inference request.
// @@
type ModelInferRequest_InferInputTensor struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@    .. cpp:var:: string name
	// @@
	// @@       The tensor name.
	// @@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// @@    .. cpp:var:: string datatype
	// @@
	// @@       The tensor data type.
	// @@
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	// @@    .. cpp:var:: repeated int64 shape
	// @@
	// @@       The tensor shape.
	// @@
	Shape []int64 `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	// @@    .. cpp:var:: map<string,InferParameter> parameters
	// @@
	// @@       Optional inference input tensor parameters.
	// @@
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// @@    .. cpp:var:: InferTensorContents contents
	// @@
	// @@       The tensor contents using a data-type format. This field
	// @@       must not be specified if "raw" tensor contents are being
	// @@       used for the inference request.
	// @@
	Contents      *InferTensorContents `protobuf:"bytes,5,opt,name=contents,proto3" json:"contents,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelInferRequest_InferInputTensor) Reset() {
	*x = ModelInferRequest_InferInputTensor{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[16]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInferRequest_InferInputTensor) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInferRequest_InferInputTensor) ProtoMessage() {}

func (x *ModelInferRequest_InferInputTensor) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[16]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInferRequest_InferInputTensor.ProtoReflect.Descriptor instead.
func (*ModelInferRequest_InferInputTensor) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{10, 1}
}

func (x *ModelInferRequest_InferInputTensor) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelInferRequest_InferInputTensor) GetDatatype() string {
	if x != nil {
		return x.Datatype
	}
	return ""
}

func (x *ModelInferRequest_InferInputTensor) GetShape() []int64 {
	if x != nil {
		return x.Shape
	}
	return nil
}

func (x *ModelInferRequest_InferInputTensor) GetParameters() map[string]*InferParameter {
	if x != nil {
		return x.Parameters
	}
	return nil
}

func (x *ModelInferRequest_InferInputTensor) GetContents() *InferTensorContents {
	if x != nil {
		return x.Contents
	}
	return nil
}

// @@
// @@  .. cpp:var:: message InferRequestedOutputTensor
// @@
// @@     An output tensor requested for an inference request.
// @@
type ModelInferRequest_InferRequestedOutputTensor struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@    .. cpp:var:: string name
	// @@
	// @@       The tensor name.
	// @@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// @@    .. cpp:var:: map<string,InferParameter> parameters
	// @@
	// @@       Optional requested output tensor parameters.
	// @@
	Parameters    map[string]*InferParameter `protobuf:"bytes,2,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelInferRequest_InferRequestedOutputTensor) Reset() {
	*x = ModelInferRequest_InferRequestedOutputTensor{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[17]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInferRequest_InferRequestedOutputTensor) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInferRequest_InferRequestedOutputTensor) ProtoMessage() {}

func (x *ModelInferRequest_InferRequestedOutputTensor) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[17]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInferRequest_InferRequestedOutputTensor.ProtoReflect.Descriptor instead.
func (*ModelInferRequest_InferRequestedOutputTensor) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{10, 2}
}

func (x *ModelInferRequest_InferRequestedOutputTensor) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelInferRequest_InferRequestedOutputTensor) GetParameters() map[string]*InferParameter {
	if x != nil {
		return x.Parameters
	}
	return nil
}

// @@
// @@  .. cpp:var:: message InferOutputTensor
// @@
// @@     An output tensor returned for an inference request.
// @@
type ModelInferResponse_InferOutputTensor struct {
	state protoimpl.MessageState `protogen:"open.v1"`
	// @@    .. cpp:var:: string name
	// @@
	// @@       The tensor name.
	// @@
	Name string `protobuf:"bytes,1,opt,name=name,proto3" json:"name,omitempty"`
	// @@    .. cpp:var:: string datatype
	// @@
	// @@       The tensor data type.
	// @@
	Datatype string `protobuf:"bytes,2,opt,name=datatype,proto3" json:"datatype,omitempty"`
	// @@    .. cpp:var:: repeated int64 shape
	// @@
	// @@       The tensor shape.
	// @@
	Shape []int64 `protobuf:"varint,3,rep,packed,name=shape,proto3" json:"shape,omitempty"`
	// @@    .. cpp:var:: map<string,InferParameter> parameters
	// @@
	// @@       Optional output tensor parameters.
	// @@
	Parameters map[string]*InferParameter `protobuf:"bytes,4,rep,name=parameters,proto3" json:"parameters,omitempty" protobuf_key:"bytes,1,opt,name=key" protobuf_val:"bytes,2,opt,name=value"`
	// @@    .. cpp:var:: InferTensorContents contents
	// @@
	// @@       The tensor contents using a data-type format. This field
	// @@       must not be specified if "raw" tensor contents are being
	// @@       used for the inference response.
	// @@
	Contents      *InferTensorContents `protobuf:"bytes,5,opt,name=contents,proto3" json:"contents,omitempty"`
	unknownFields protoimpl.UnknownFields
	sizeCache     protoimpl.SizeCache
}

func (x *ModelInferResponse_InferOutputTensor) Reset() {
	*x = ModelInferResponse_InferOutputTensor{}
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[21]
	ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
	ms.StoreMessageInfo(mi)
}

func (x *ModelInferResponse_InferOutputTensor) String() string {
	return protoimpl.X.MessageStringOf(x)
}

func (*ModelInferResponse_InferOutputTensor) ProtoMessage() {}

func (x *ModelInferResponse_InferOutputTensor) ProtoReflect() protoreflect.Message {
	mi := &file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[21]
	if x != nil {
		ms := protoimpl.X.MessageStateOf(protoimpl.Pointer(x))
		if ms.LoadMessageInfo() == nil {
			ms.StoreMessageInfo(mi)
		}
		return ms
	}
	return mi.MessageOf(x)
}

// Deprecated: Use ModelInferResponse_InferOutputTensor.ProtoReflect.Descriptor instead.
func (*ModelInferResponse_InferOutputTensor) Descriptor() ([]byte, []int) {
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP(), []int{11, 1}
}

func (x *ModelInferResponse_InferOutputTensor) GetName() string {
	if x != nil {
		return x.Name
	}
	return ""
}

func (x *ModelInferResponse_InferOutputTensor) GetDatatype() string {
	if x != nil {
		return x.Datatype
	}
	return ""
}

func (x *ModelInferResponse_InferOutputTensor) GetShape() []int64 {
	if x != nil {
		return x.Shape
	}
	return nil
}

func (x *ModelInferResponse_InferOutputTensor) GetParameters() map[string]*InferParameter {
	if x != nil {
		return x.Parameters
	}
	return nil
}

func (x *ModelInferResponse_InferOutputTensor) GetContents() *InferTensorContents {
	if x != nil {
		return x.Contents
	}
	return nil
}

var File_pkg_proto_inference_v2_grpc_service_proto protoreflect.FileDescriptor

const file_pkg_proto_inference_v2_grpc_service_proto_rawDesc = "" +
	"\n" +
	")pkg/proto/inference/v2/grpc_service.proto\x12\tinference\"\x13\n" +
	"\x11ServerLiveRequest\"(\n" +
	"\x12ServerLiveResponse\x12\x12\n" +
	"\x04live\x18\x01 \x01(\bR\x04live\"\x14\n" +
	"\x12ServerReadyRequest\"+\n" +
	"\x13ServerReadyResponse\x12\x14\n" +
	"\x05ready\x18\x01 \x01(\bR\x05ready\"A\n" +
	"\x11ModelReadyRequest\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x18\n" +
	"\aversion\x18\x02 \x01(\tR\aversion\"*\n" +
	"\x12ModelReadyResponse\x12\x14\n" +
	"\x05ready\x18\x01 \x01(\bR\x05ready\"\x17\n" +
	"\x15ServerMetadataRequest\"f\n" +
	"\x16ServerMetadataResponse\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x18\n" +
	"\aversion\x18\x02 \x01(\tR\aversion\x12\x1e\n" +
	"\n" +
	"extensions\x18\x03 \x03(\tR\n" +
	"extensions\"D\n" +
	"\x14ModelMetadataRequest\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x18\n" +
	"\aversion\x18\x02 \x01(\tR\aversion\"\xcb\x01\n" +
	"\x15ModelMetadataResponse\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1a\n" +
	"\bversions\x18\x02 \x03(\tR\bversions\x12\x1a\n" +
	"\bplatform\x18\x03 \x01(\tR\bplatform\x121\n" +
	"\x06inputs\x18\x04 \x03(\v2\x19.inference.TensorMetadataR\x06inputs\x123\n" +
	"\aoutputs\x18\x05 \x03(\v2\x19.inference.TensorMetadataR\aoutputs\"\x9d\b\n" +
	"\x11ModelInferRequest\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12#\n" +
	"\rmodel_version\x18\x02 \x01(\tR\fmodelVersion\x12\x0e\n" +
	"\x02id\x18\x03 \x01(\tR\x02id\x12L\n" +
	"\n" +
	"parameters\x18\x04 \x03(\v2,.inference.ModelInferRequest.ParametersEntryR\n" +
	"parameters\x12E\n" +
	"\x06inputs\x18\x05 \x03(\v2-.inference.ModelInferRequest.InferInputTensorR\x06inputs\x12Q\n" +
	"\aoutputs\x18\x06 \x03(\v27.inference.ModelInferRequest.InferRequestedOutputTensorR\aoutputs\x12,\n" +
	"\x12raw_input_contents\x18\a \x03(\fR\x10rawInputContents\x1aX\n" +
	"\x0fParametersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12/\n" +
	"\x05value\x18\x02 \x01(\v2\x19.inference.InferParameterR\x05value:\x028\x01\x1a\xcd\x02\n" +
	"\x10InferInputTensor\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1a\n" +
	"\bdatatype\x18\x02 \x01(\tR\bdatatype\x12\x14\n" +
	"\x05shape\x18\x03 \x03(\x03R\x05shape\x12]\n" +
	"\n" +
	"parameters\x18\x04 \x03(\v2=.inference.ModelInferRequest.InferInputTensor.ParametersEntryR\n" +
	"parameters\x12:\n" +
	"\bcontents\x18\x05 \x01(\v2\x1e.inference.InferTensorContentsR\bcontents\x1aX\n" +
	"\x0fParametersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12/\n" +
	"\x05value\x18\x02 \x01(\v2\x19.inference.InferParameterR\x05value:\x028\x01\x1a\xf3\x01\n" +
	"\x1aInferRequestedOutputTensor\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12g\n" +
	"\n" +
	"parameters\x18\x02 \x03(\v2G.inference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntryR\n" +
	"parameters\x1aX\n" +
	"\x0fParametersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12/\n" +
	"\x05value\x18\x02 \x01(\v2\x19.inference.InferParameterR\x05value:\x028\x01\"\xdf\x05\n" +
	"\x12ModelInferResponse\x12\x1d\n" +
	"\n" +
	"model_name\x18\x01 \x01(\tR\tmodelName\x12#\n" +
	"\rmodel_version\x18\x02 \x01(\tR\fmodelVersion\x12\x0e\n" +
	"\x02id\x18\x03 \x01(\tR\x02id\x12M\n" +
	"\n" +
	"parameters\x18\x04 \x03(\v2-.inference.ModelInferResponse.ParametersEntryR\n" +
	"parameters\x12I\n" +
	"\aoutputs\x18\x05 \x03(\v2/.inference.ModelInferResponse.InferOutputTensorR\aoutputs\x12.\n" +
	"\x13raw_output_contents\x18\x06 \x03(\fR\x11rawOutputContents\x1aX\n" +
	"\x0fParametersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12/\n" +
	"\x05value\x18\x02 \x01(\v2\x19.inference.InferParameterR\x05value:\x028\x01\x1a\xd0\x02\n" +
	"\x11InferOutputTensor\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1a\n" +
	"\bdatatype\x18\x02 \x01(\tR\bdatatype\x12\x14\n" +
	"\x05shape\x18\x03 \x03(\x03R\x05shape\x12_\n" +
	"\n" +
	"parameters\x18\x04 \x03(\v2?.inference.ModelInferResponse.InferOutputTensor.ParametersEntryR\n" +
	"parameters\x12:\n" +
	"\bcontents\x18\x05 \x01(\v2\x1e.inference.InferTensorContentsR\bcontents\x1aX\n" +
	"\x0fParametersEntry\x12\x10\n" +
	"\x03key\x18\x01 \x01(\tR\x03key\x12/\n" +
	"\x05value\x18\x02 \x01(\v2\x19.inference.InferParameterR\x05value:\x028\x01\"V\n" +
	"\x0eTensorMetadata\x12\x12\n" +
	"\x04name\x18\x01 \x01(\tR\x04name\x12\x1a\n" +
	"\bdatatype\x18\x02 \x01(\tR\bdatatype\x12\x14\n" +
	"\x05shape\x18\x03 \x03(\x03R\x05shape\"\x8d\x01\n" +
	"\x0eInferParameter\x12\x1f\n" +
	"\n" +
	"bool_param\x18\x01 \x01(\bH\x00R\tboolParam\x12!\n" +
	"\vint64_param\x18\x02 \x01(\x03H\x00R\n" +
	"int64Param\x12#\n" +
	"\fstring_param\x18\x03 \x01(\tH\x00R\vstringParamB\x12\n" +
	"\x10parameter_choice\"\xc3\x02\n" +
	"\x13InferTensorContents\x12#\n" +
	"\rbool_contents\x18\x01 \x03(\bR\fboolContents\x12!\n" +
	"\fint_contents\x18\x02 \x03(\x05R\vintContents\x12%\n" +
	"\x0eint64_contents\x18\x03 \x03(\x03R\rint64Contents\x12#\n" +
	"\ruint_contents\x18\x04 \x03(\rR\fuintContents\x12'\n" +
	"\x0fuint64_contents\x18\x05 \x03(\x04R\x0euint64Contents\x12#\n" +
	"\rfp32_contents\x18\x06 \x03(\x02R\ffp32Contents\x12#\n" +
	"\rfp64_contents\x18\a \x03(\x01R\ffp64Contents\x12%\n" +
	"\x0ebytes_contents\x18\b \x03(\fR\rbytesContents2\xfc\x03\n" +
	"\x14GRPCInferenceService\x12K\n" +
	"\n" +
	"ServerLive\x12\x1c.inference.ServerLiveRequest\x1a\x1d.inference.ServerLiveResponse\"\x00\x12N\n" +
	"\vServerReady\x12\x1d.inference.ServerReadyRequest\x1a\x1e.inference.ServerReadyResponse\"\x00\x12K\n" +
	"\n" +
	"ModelReady\x12\x1c.inference.ModelReadyRequest\x1a\x1d.inference.ModelReadyResponse\"\x00\x12W\n" +
	"\x0eServerMetadata\x12 .inference.ServerMetadataRequest\x1a!.inference.ServerMetadataResponse\"\x00\x12T\n" +
	"\rModelMetadata\x12\x1f.inference.ModelMetadataRequest\x1a .inference.ModelMetadataResponse\"\x00\x12K\n" +
	"\n" +
	"ModelInfer\x12\x1c.inference.ModelInferRequest\x1a\x1d.inference.ModelInferResponse\"\x00BTZRgithub.com/scc-digitalhub/digitalhub-servicegraph/pkg/proto/inference/v2;inferenceb\x06proto3"

var (
	file_pkg_proto_inference_v2_grpc_service_proto_rawDescOnce sync.Once
	file_pkg_proto_inference_v2_grpc_service_proto_rawDescData []byte
)

func file_pkg_proto_inference_v2_grpc_service_proto_rawDescGZIP() []byte {
	file_pkg_proto_inference_v2_grpc_service_proto_rawDescOnce.Do(func() {
		file_pkg_proto_inference_v2_grpc_service_proto_rawDescData = protoimpl.X.CompressGZIP(unsafe.Slice(unsafe.StringData(file_pkg_proto_inference_v2_grpc_service_proto_rawDesc), len(file_pkg_proto_inference_v2_grpc_service_proto_rawDesc)))
	})
	return file_pkg_proto_inference_v2_grpc_service_proto_rawDescData
}

var file_pkg_proto_inference_v2_grpc_service_proto_msgTypes = make([]protoimpl.MessageInfo, 23)
var file_pkg_proto_inference_v2_grpc_service_proto_goTypes = []any{
	(*ServerLiveRequest)(nil),                            // 0: inference.ServerLiveRequest
	(*ServerLiveResponse)(nil),                           // 1: inference.ServerLiveResponse
	(*ServerReadyRequest)(nil),                           // 2: inference.ServerReadyRequest
	(*ServerReadyResponse)(nil),                          // 3: inference.ServerReadyResponse
	(*ModelReadyRequest)(nil),                            // 4: inference.ModelReadyRequest
	(*ModelReadyResponse)(nil),                           // 5: inference.ModelReadyResponse
	(*ServerMetadataRequest)(nil),                        // 6: inference.ServerMetadataRequest
	(*ServerMetadataResponse)(nil),                       // 7: inference.ServerMetadataResponse
	(*ModelMetadataRequest)(nil),                         // 8: inference.ModelMetadataRequest
	(*ModelMetadataResponse)(nil),                        // 9: inference.ModelMetadataResponse
	(*ModelInferRequest)(nil),                            // 10: inference.ModelInferRequest
	(*ModelInferResponse)(nil),                           // 11: inference.ModelInferResponse
	(*TensorMetadata)(nil),                               // 12: inference.TensorMetadata
	(*InferParameter)(nil),                               // 13: inference.InferParameter
	(*InferTensorContents)(nil),                          // 14: inference.InferTensorContents
	nil,                                                  // 15: inference.ModelInferRequest.ParametersEntry
	(*ModelInferRequest_InferInputTensor)(nil),           // 16: inference.ModelInferRequest.InferInputTensor
	(*ModelInferRequest_InferRequestedOutputTensor)(nil), // 17: inference.ModelInferRequest.InferRequestedOutputTensor
	nil, // 18: inference.ModelInferRequest.InferInputTensor.ParametersEntry
	nil, // 19: inference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry
	nil, // 20: inference.ModelInferResponse.ParametersEntry
	(*ModelInferResponse_InferOutputTensor)(nil), // 21: inference.ModelInferResponse.InferOutputTensor
	nil, // 22: inference.ModelInferResponse.InferOutputTensor.ParametersEntry
}
var file_pkg_proto_inference_v2_grpc_service_proto_depIdxs = []int32{
	12, // 0: inference.ModelMetadataResponse.inputs:type_name -> inference.TensorMetadata
	12, // 1: inference.ModelMetadataResponse.outputs:type_name -> inference.TensorMetadata
	15, // 2: inference.ModelInferRequest.parameters:type_name -> inference.ModelInferRequest.ParametersEntry
	16, // 3: inference.ModelInferRequest.inputs:type_name -> inference.ModelInferRequest.InferInputTensor
	17, // 4: inference.ModelInferRequest.outputs:type_name -> inference.ModelInferRequest.InferRequestedOutputTensor
	20, // 5: inference.ModelInferResponse.parameters:type_name -> inference.ModelInferResponse.ParametersEntry
	21, // 6: inference.ModelInferResponse.outputs:type_name -> inference.ModelInferResponse.InferOutputTensor
	13, // 7: inference.ModelInferRequest.ParametersEntry.value:type_name -> inference.InferParameter
	18, // 8: inference.ModelInferRequest.InferInputTensor.parameters:type_name -> inference.ModelInferRequest.InferInputTensor.ParametersEntry
	14, // 9: inference.ModelInferRequest.InferInputTensor.contents:type_name -> inference.InferTensorContents
	19, // 10: inference.ModelInferRequest.InferRequestedOutputTensor.parameters:type_name -> inference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry
	13, // 11: inference.ModelInferRequest.InferInputTensor.ParametersEntry.value:type_name -> inference.InferParameter
	13, // 12: inference.ModelInferRequest.InferRequestedOutputTensor.ParametersEntry.value:type_name -> inference.InferParameter
	13, // 13: inference.ModelInferResponse.ParametersEntry.value:type_name -> inference.InferParameter
	22, // 14: inference.ModelInferResponse.InferOutputTensor.parameters:type_name -> inference.ModelInferResponse.InferOutputTensor.ParametersEntry
	14, // 15: inference.ModelInferResponse.InferOutputTensor.contents:type_name -> inference.InferTensorContents
	13, // 16: inference.ModelInferResponse.InferOutputTensor.ParametersEntry.value:type_name -> inference.InferParameter
	0,  // 17: inference.GRPCInferenceService.ServerLive:input_type -> inference.ServerLiveRequest
	2,  // 18: inference.GRPCInferenceService.ServerReady:input_type -> inference.ServerReadyRequest
	4,  // 19: inference.GRPCInferenceService.ModelReady:input_type -> inference.ModelReadyRequest
	6,  // 20: inference.GRPCInferenceService.ServerMetadata:input_type -> inference.ServerMetadataRequest
	8,  // 21: inference.GRPCInferenceService.ModelMetadata:input_type -> inference.ModelMetadataRequest
	10, // 22: inference.GRPCInferenceService.ModelInfer:input_type -> inference.ModelInferRequest
	1,  // 23: inference.GRPCInferenceService.ServerLive:output_type -> inference.ServerLiveResponse
	3,  // 24: inference.GRPCInferenceService.ServerReady:output_type -> inference.ServerReadyResponse
	5,  // 25: inference.GRPCInferenceService.ModelReady:output_type -> inference.ModelReadyResponse
	7,  // 26: inference.GRPCInferenceService.ServerMetadata:output_type -> inference.ServerMetadataResponse
	9,  // 27: inference.GRPCInferenceService.ModelMetadata:output_type -> inference.ModelMetadataResponse
	11, // 28: inference.GRPCInferenceService.ModelInfer:output_type -> inference.ModelInferResponse
	23, // [23:29] is the sub-list for method output_type
	17, // [17:23] is the sub-list for method input_type
	17, // [17:17] is the sub-list for extension type_name
	17, // [17:17] is the sub-list for extension extendee
	0,  // [0:17] is the sub-list for field type_name
}

func init() { file_pkg_proto_inference_v2_grpc_service_proto_init() }
func file_pkg_proto_inference_v2_grpc_service_proto_init() {
	if File_pkg_proto_inference_v2_grpc_service_proto != nil {
		return
	}
	file_pkg_proto_inference_v2_grpc_service_proto_msgTypes[13].OneofWrappers = []any{
		(*InferParameter_BoolParam)(nil),
		(*InferParameter_Int64Param)(nil),
		(*InferParameter_StringParam)(nil),
	}
	type x struct{}
	out := protoimpl.TypeBuilder{
		File: protoimpl.DescBuilder{
			GoPackagePath: reflect.TypeOf(x{}).PkgPath(),
			RawDescriptor: unsafe.Slice(unsafe.StringData(file_pkg_proto_inference_v2_grpc_service_proto_rawDesc), len(file_pkg_proto_inference_v2_grpc_service_proto_rawDesc)),
			NumEnums:      0,
			NumMessages:   23,
			NumExtensions: 0,
			NumServices:   1,
		},
		GoTypes:           file_pkg_proto_inference_v2_grpc_service_proto_goTypes,
		DependencyIndexes: file_pkg_proto_inference_v2_grpc_service_proto_depIdxs,
		MessageInfos:      file_pkg_proto_inference_v2_grpc_service_proto_msgTypes,
	}.Build()
	File_pkg_proto_inference_v2_grpc_service_proto = out.File
	file_pkg_proto_inference_v2_grpc_service_proto_goTypes = nil
	file_pkg_proto_inference_v2_grpc_service_proto_depIdxs = nil
}
